{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de10313c-5b51-484d-9877-a8b2a32585df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fastprogress.fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "547335f1-ea03-446e-8c98-b46e3d376939",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/small_corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "590619d0-b203-49db-98ef-21dcc30ef903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fastprogress -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2eedf77-aed8-4b68-847e-5c361b0bb65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ratings</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122468</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently UBISOFT had to settle a huge class-ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>495370</td>\n",
       "      <td>1</td>\n",
       "      <td>code didn't work, got me a refund.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214717</td>\n",
       "      <td>1</td>\n",
       "      <td>these do not work at all, all i get is static ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>231734</td>\n",
       "      <td>1</td>\n",
       "      <td>well let me start by saying that when i first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154709</td>\n",
       "      <td>1</td>\n",
       "      <td>Dont waste your money, you will just end up us...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ratings                                            reviews\n",
       "0      122468        1  Recently UBISOFT had to settle a huge class-ac...\n",
       "1      495370        1                 code didn't work, got me a refund.\n",
       "2      214717        1  these do not work at all, all i get is static ...\n",
       "3      231734        1  well let me start by saying that when i first ...\n",
       "4      154709        1  Dont waste your money, you will just end up us..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2865544-b8d8-4e63-9cb9-b0f2c2fd3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_to_Target(value):\n",
    "    if value >= 5:\n",
    "        return 2\n",
    "    if value <= 4 and value >= 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "df['labels'] = df['ratings'].apply(lambda x:score_to_Target(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0653e665-1701-4d22-9a80-36e8fb75f930",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset = [\"reviews\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f3448a5-bdbf-4863-ac1b-3d6f73d01000",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df.reviews.values\n",
    "labels = df.labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c17f646e-cb09-489d-b8e1-26b84dc57188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4d43df4-f599-41da-8562-aa546487b0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (667 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  6767\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19080e65-d116-4da1-8b67-09cf18c6f613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4883b49-ba35-4971-910b-6e61ba63d3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  code didn't work, got me a refund.\n",
      "Token IDs: tensor([  101,  3642,  2134,  1005,  1056,  2147,  1010,  2288,  2033,  1037,\n",
      "        25416,  8630,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-694ca8ba0388>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels)\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_mask = []\n",
    "for sent in sentences:\n",
    "    token_dict = tokenizer.encode_plus(\n",
    "        sent,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding = 'max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=True\n",
    "    )\n",
    "    input_ids.append(token_dict['input_ids'])\n",
    "    attention_mask.append(token_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_mask, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "print('Original: ', sentences[1])\n",
    "print('Token IDs:', input_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba1db6a8-a3f9-457d-8940-fa2dc4b43f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset,random_split\n",
    "dataset = TensorDataset(input_ids,attention_masks,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cfd60f8-3486-4148-b964-1c66bc4ebb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14019145-a6da-4fc1-88e2-9d2e3de26681",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bacc318c-94c0-430f-ac82-e873e82ae1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "42fc9cb7-8069-4eb5-9761-123ad8f6f5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 3, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 3e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "89a29cd6-9e18-49cf-bc94-cd52e5bbb57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "187209aa-50d8-4be2-b4db-34c2a9183edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "953da7e4-02df-45f2-822b-ef429bf3ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs,model,train_dl,valid_dl,opt):\n",
    "    mb = master_bar(range(epochs))\n",
    "    mb.write(['epoch','train_loss','valid_loss','trn_acc','val_acc'],table=True)\n",
    "\n",
    "    for i in mb:    \n",
    "        trn_loss,val_loss = 0.0,0.0\n",
    "        trn_acc,val_acc = 0,0\n",
    "        trn_n,val_n = len(train_dl.dataset),len(valid_dl.dataset)\n",
    "        model.train()\n",
    "        for input_ids,mask,labels in progress_bar(train_dl,parent=mb):\n",
    "            input_ids,mask,labels = input_ids.to(device), mask.to(device),labels.to(device)\n",
    "            out = model(input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=mask, \n",
    "                       labels=labels,\n",
    "                       return_dict=True)\n",
    "            opt.zero_grad()\n",
    "            loss = out[0]\n",
    "            _,pred = torch.max(out[1], 1)\n",
    "            trn_acc += (pred == labels).sum().item()\n",
    "            trn_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "        trn_loss /= mb.child.total\n",
    "        trn_acc /= trn_n\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for input_ids,mask,labels in progress_bar(valid_dl,parent=mb):\n",
    "                input_ids,mask,labels = input_ids.to(device), mask.to(device),labels.to(device)\n",
    "                result = model(input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=mask,\n",
    "                           labels=labels,\n",
    "                           return_dict=True)\n",
    "                loss = result.loss\n",
    "                logits = result.logits\n",
    "                val_loss += loss.item()\n",
    "                _,pred = torch.max(logits, 1)\n",
    "                val_acc += (pred == labels).sum().item()\n",
    "        val_loss /= mb.child.total\n",
    "        val_acc /= val_n\n",
    "\n",
    "        mb.write([i,f'{trn_loss:.6f}',f'{val_loss:.6f}',f'{trn_acc:.6f}',f'{val_acc:.6f}'],table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "34ca3e14-35fa-4f32-9049-61e87be30ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>trn_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.820737</td>\n",
       "      <td>0.609056</td>\n",
       "      <td>0.628797</td>\n",
       "      <td>0.737778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.545090</td>\n",
       "      <td>0.598580</td>\n",
       "      <td>0.775253</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.366263</td>\n",
       "      <td>0.649606</td>\n",
       "      <td>0.864658</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit(3,model=model,train_dl=train_dataloader,valid_dl=validation_dataloader,opt=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f67a1572-b611-471d-9383-5ce6bdb556d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/tokenizer_config.json',\n",
       " './model_save/special_tokens_map.json',\n",
       " './model_save/vocab.txt',\n",
       " './model_save/added_tokens.json')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "output_dir = './model_save/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e3c530-8d3b-45e0-9df8-8168fe2c960b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
